{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cschu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac50a31eb520>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load bill status data to obtain the status\n",
    "\n",
    "with open('..\\\\Data\\\\107th-112th Congress\\\\Bill status\\\\HR_bill_status_contemporary.json') as f:\n",
    "    HR_data1= json.load(f)\n",
    "\n",
    "with open('..\\\\Data\\\\107th-112th Congress\\\\Bill status\\\\Sen_bill_status_contemporary.json') as f:\n",
    "    Sen_data1= json.load(f)\n",
    "    \n",
    "with open('..\\\\Data\\\\113th-114th Congress\\\\Bill status\\\\HR_bill_status_modern.json') as f:\n",
    "    HR_data2= json.load(f)\n",
    "\n",
    "with open('..\\\\Data\\\\113th-114th Congress\\\\Bill status\\\\Sen_bill_status_modern.json') as f:\n",
    "    Sen_data2= json.load(f)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_status(bill_list):\n",
    "    v = {}\n",
    "    for b in bill_list:\n",
    "        v.update({b['bill_id']:b['status']})\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bill id and status\n",
    "\n",
    "d1 = extract_id_status(HR_data1)\n",
    "d2 = extract_id_status(Sen_data1)\n",
    "d3 = extract_id_status(HR_data2)\n",
    "d4 = extract_id_status(Sen_data2)\n",
    "\n",
    "#Concatenate dictionaries into one\n",
    "status_dict = {}\n",
    "for d in (d1,d2,d3,d4):\n",
    "    status_dict.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr1-107': 'ENACTED:SIGNED',\n",
       " 'hr10-107': 'ENACTED:SIGNED',\n",
       " 'hr100-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1000-107': 'ENACTED:SIGNED',\n",
       " 'hr1001-107': 'REFERRED',\n",
       " 'hr1002-107': 'REFERRED',\n",
       " 'hr1003-107': 'REFERRED',\n",
       " 'hr1004-107': 'REFERRED',\n",
       " 'hr1005-107': 'REFERRED',\n",
       " 'hr1006-107': 'REFERRED',\n",
       " 'hr1007-107': 'REPORTED',\n",
       " 'hr1008-107': 'REFERRED',\n",
       " 'hr1009-107': 'PASS_OVER:HOUSE',\n",
       " 'hr101-107': 'REFERRED',\n",
       " 'hr1010-107': 'REFERRED',\n",
       " 'hr1011-107': 'REFERRED',\n",
       " 'hr1012-107': 'REFERRED',\n",
       " 'hr1013-107': 'REFERRED',\n",
       " 'hr1014-107': 'REFERRED',\n",
       " 'hr1015-107': 'REFERRED',\n",
       " 'hr1016-107': 'REFERRED',\n",
       " 'hr1017-107': 'REFERRED',\n",
       " 'hr1018-107': 'REFERRED',\n",
       " 'hr1019-107': 'REFERRED',\n",
       " 'hr102-107': 'REFERRED',\n",
       " 'hr1020-107': 'REPORTED',\n",
       " 'hr1021-107': 'REFERRED',\n",
       " 'hr1022-107': 'REPORTED',\n",
       " 'hr1023-107': 'REFERRED',\n",
       " 'hr1024-107': 'REFERRED',\n",
       " 'hr1025-107': 'REFERRED',\n",
       " 'hr1026-107': 'REFERRED',\n",
       " 'hr1027-107': 'REFERRED',\n",
       " 'hr1028-107': 'REFERRED',\n",
       " 'hr1029-107': 'REFERRED',\n",
       " 'hr103-107': 'REFERRED',\n",
       " 'hr1030-107': 'REFERRED',\n",
       " 'hr1031-107': 'REFERRED',\n",
       " 'hr1032-107': 'REFERRED',\n",
       " 'hr1033-107': 'REFERRED',\n",
       " 'hr1034-107': 'REFERRED',\n",
       " 'hr1035-107': 'REFERRED',\n",
       " 'hr1036-107': 'REFERRED',\n",
       " 'hr1037-107': 'REFERRED',\n",
       " 'hr1038-107': 'REFERRED',\n",
       " 'hr1039-107': 'REFERRED',\n",
       " 'hr104-107': 'REFERRED',\n",
       " 'hr1040-107': 'REFERRED',\n",
       " 'hr1041-107': 'REFERRED',\n",
       " 'hr1042-107': 'ENACTED:SIGNED',\n",
       " 'hr1043-107': 'REFERRED',\n",
       " 'hr1044-107': 'REFERRED',\n",
       " 'hr1045-107': 'REFERRED',\n",
       " 'hr1046-107': 'REFERRED',\n",
       " 'hr1047-107': 'REFERRED',\n",
       " 'hr1048-107': 'REFERRED',\n",
       " 'hr1049-107': 'REFERRED',\n",
       " 'hr105-107': 'REFERRED',\n",
       " 'hr1050-107': 'REFERRED',\n",
       " 'hr1051-107': 'REFERRED',\n",
       " 'hr1052-107': 'REFERRED',\n",
       " 'hr1053-107': 'REFERRED',\n",
       " 'hr1054-107': 'REFERRED',\n",
       " 'hr1055-107': 'REFERRED',\n",
       " 'hr1056-107': 'REFERRED',\n",
       " 'hr1057-107': 'REFERRED',\n",
       " 'hr1058-107': 'REFERRED',\n",
       " 'hr1059-107': 'REFERRED',\n",
       " 'hr106-107': 'REFERRED',\n",
       " 'hr1060-107': 'REFERRED',\n",
       " 'hr1061-107': 'REFERRED',\n",
       " 'hr1062-107': 'REFERRED',\n",
       " 'hr1063-107': 'REFERRED',\n",
       " 'hr1064-107': 'REFERRED',\n",
       " 'hr1065-107': 'REFERRED',\n",
       " 'hr1066-107': 'REFERRED',\n",
       " 'hr1067-107': 'REFERRED',\n",
       " 'hr1068-107': 'REFERRED',\n",
       " 'hr1069-107': 'REFERRED',\n",
       " 'hr107-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1070-107': 'ENACTED:SIGNED',\n",
       " 'hr1071-107': 'REFERRED',\n",
       " 'hr1072-107': 'REFERRED',\n",
       " 'hr1073-107': 'REFERRED',\n",
       " 'hr1074-107': 'REFERRED',\n",
       " 'hr1075-107': 'REFERRED',\n",
       " 'hr1076-107': 'REFERRED',\n",
       " 'hr1077-107': 'REFERRED',\n",
       " 'hr1078-107': 'REFERRED',\n",
       " 'hr1079-107': 'REFERRED',\n",
       " 'hr108-107': 'REFERRED',\n",
       " 'hr1080-107': 'REFERRED',\n",
       " 'hr1081-107': 'REFERRED',\n",
       " 'hr1082-107': 'REFERRED',\n",
       " 'hr1083-107': 'REFERRED',\n",
       " 'hr1084-107': 'REFERRED',\n",
       " 'hr1085-107': 'REFERRED',\n",
       " 'hr1086-107': 'REFERRED',\n",
       " 'hr1087-107': 'REFERRED',\n",
       " 'hr1088-107': 'ENACTED:SIGNED',\n",
       " 'hr1089-107': 'REFERRED',\n",
       " 'hr109-107': 'REFERRED',\n",
       " 'hr1090-107': 'REFERRED',\n",
       " 'hr1091-107': 'REFERRED',\n",
       " 'hr1092-107': 'REFERRED',\n",
       " 'hr1093-107': 'REFERRED',\n",
       " 'hr1094-107': 'REFERRED',\n",
       " 'hr1095-107': 'REFERRED',\n",
       " 'hr1096-107': 'REFERRED',\n",
       " 'hr1097-107': 'REFERRED',\n",
       " 'hr1098-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1099-107': 'PASS_OVER:HOUSE',\n",
       " 'hr11-107': 'REFERRED',\n",
       " 'hr110-107': 'REFERRED',\n",
       " 'hr1100-107': 'REFERRED',\n",
       " 'hr1101-107': 'REFERRED',\n",
       " 'hr1102-107': 'REFERRED',\n",
       " 'hr1103-107': 'REFERRED',\n",
       " 'hr1104-107': 'REFERRED',\n",
       " 'hr1105-107': 'REFERRED',\n",
       " 'hr1106-107': 'REFERRED',\n",
       " 'hr1107-107': 'REFERRED',\n",
       " 'hr1108-107': 'REFERRED',\n",
       " 'hr1109-107': 'REFERRED',\n",
       " 'hr111-107': 'REFERRED',\n",
       " 'hr1110-107': 'REFERRED',\n",
       " 'hr1111-107': 'REFERRED',\n",
       " 'hr1112-107': 'REFERRED',\n",
       " 'hr1113-107': 'REFERRED',\n",
       " 'hr1114-107': 'REFERRED',\n",
       " 'hr1115-107': 'REFERRED',\n",
       " 'hr1116-107': 'REFERRED',\n",
       " 'hr1117-107': 'REFERRED',\n",
       " 'hr1118-107': 'REFERRED',\n",
       " 'hr1119-107': 'REFERRED',\n",
       " 'hr112-107': 'REFERRED',\n",
       " 'hr1120-107': 'REFERRED',\n",
       " 'hr1121-107': 'REFERRED',\n",
       " 'hr1122-107': 'REFERRED',\n",
       " 'hr1123-107': 'REFERRED',\n",
       " 'hr1124-107': 'REFERRED',\n",
       " 'hr1125-107': 'REFERRED',\n",
       " 'hr1126-107': 'REFERRED',\n",
       " 'hr1127-107': 'REFERRED',\n",
       " 'hr1128-107': 'REFERRED',\n",
       " 'hr1129-107': 'REFERRED',\n",
       " 'hr113-107': 'REFERRED',\n",
       " 'hr1130-107': 'REFERRED',\n",
       " 'hr1131-107': 'REFERRED',\n",
       " 'hr1132-107': 'REFERRED',\n",
       " 'hr1133-107': 'REFERRED',\n",
       " 'hr1134-107': 'REFERRED',\n",
       " 'hr1135-107': 'REFERRED',\n",
       " 'hr1136-107': 'REFERRED',\n",
       " 'hr1137-107': 'REFERRED',\n",
       " 'hr1138-107': 'REFERRED',\n",
       " 'hr1139-107': 'REFERRED',\n",
       " 'hr114-107': 'REFERRED',\n",
       " 'hr1140-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1141-107': 'REFERRED',\n",
       " 'hr1142-107': 'REFERRED',\n",
       " 'hr1143-107': 'REFERRED',\n",
       " 'hr1144-107': 'REFERRED',\n",
       " 'hr1145-107': 'REFERRED',\n",
       " 'hr1146-107': 'REFERRED',\n",
       " 'hr1147-107': 'REFERRED',\n",
       " 'hr1148-107': 'REFERRED',\n",
       " 'hr1149-107': 'REFERRED',\n",
       " 'hr115-107': 'REFERRED',\n",
       " 'hr1150-107': 'REFERRED',\n",
       " 'hr1151-107': 'REFERRED',\n",
       " 'hr1152-107': 'REFERRED',\n",
       " 'hr1153-107': 'REFERRED',\n",
       " 'hr1154-107': 'REFERRED',\n",
       " 'hr1155-107': 'REFERRED',\n",
       " 'hr1156-107': 'REFERRED',\n",
       " 'hr1157-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1158-107': 'REFERRED',\n",
       " 'hr1159-107': 'REFERRED',\n",
       " 'hr116-107': 'REFERRED',\n",
       " 'hr1160-107': 'REFERRED',\n",
       " 'hr1161-107': 'ENACTED:SIGNED',\n",
       " 'hr1162-107': 'REFERRED',\n",
       " 'hr1163-107': 'REFERRED',\n",
       " 'hr1164-107': 'REFERRED',\n",
       " 'hr1165-107': 'REFERRED',\n",
       " 'hr1166-107': 'REFERRED',\n",
       " 'hr1167-107': 'REFERRED',\n",
       " 'hr1168-107': 'REFERRED',\n",
       " 'hr1169-107': 'REFERRED',\n",
       " 'hr117-107': 'REFERRED',\n",
       " 'hr1170-107': 'REFERRED',\n",
       " 'hr1171-107': 'REFERRED',\n",
       " 'hr1172-107': 'REFERRED',\n",
       " 'hr1173-107': 'REFERRED',\n",
       " 'hr1174-107': 'REFERRED',\n",
       " 'hr1175-107': 'REFERRED',\n",
       " 'hr1176-107': 'REFERRED',\n",
       " 'hr1177-107': 'REFERRED',\n",
       " 'hr1178-107': 'REFERRED',\n",
       " 'hr1179-107': 'REFERRED',\n",
       " 'hr118-107': 'REFERRED',\n",
       " 'hr1180-107': 'REFERRED',\n",
       " 'hr1181-107': 'REFERRED',\n",
       " 'hr1182-107': 'REFERRED',\n",
       " 'hr1183-107': 'ENACTED:SIGNED',\n",
       " 'hr1184-107': 'REFERRED',\n",
       " 'hr1185-107': 'REFERRED',\n",
       " 'hr1186-107': 'REFERRED',\n",
       " 'hr1187-107': 'REFERRED',\n",
       " 'hr1188-107': 'REFERRED',\n",
       " 'hr1189-107': 'REFERRED',\n",
       " 'hr119-107': 'REFERRED',\n",
       " 'hr1190-107': 'REFERRED',\n",
       " 'hr1191-107': 'REFERRED',\n",
       " 'hr1192-107': 'REFERRED',\n",
       " 'hr1193-107': 'REFERRED',\n",
       " 'hr1194-107': 'REFERRED',\n",
       " 'hr1195-107': 'REFERRED',\n",
       " 'hr1196-107': 'REFERRED',\n",
       " 'hr1197-107': 'REFERRED',\n",
       " 'hr1198-107': 'REFERRED',\n",
       " 'hr1199-107': 'REFERRED',\n",
       " 'hr12-107': 'REFERRED',\n",
       " 'hr120-107': 'REFERRED',\n",
       " 'hr1200-107': 'REFERRED',\n",
       " 'hr1201-107': 'REFERRED',\n",
       " 'hr1202-107': 'REFERRED',\n",
       " 'hr1203-107': 'REFERRED',\n",
       " 'hr1204-107': 'REFERRED',\n",
       " 'hr1205-107': 'REFERRED',\n",
       " 'hr1206-107': 'REFERRED',\n",
       " 'hr1207-107': 'REFERRED',\n",
       " 'hr1208-107': 'REFERRED',\n",
       " 'hr1209-107': 'ENACTED:SIGNED',\n",
       " 'hr121-107': 'REFERRED',\n",
       " 'hr1210-107': 'REFERRED',\n",
       " 'hr1211-107': 'REFERRED',\n",
       " 'hr1212-107': 'REFERRED',\n",
       " 'hr1213-107': 'REFERRED',\n",
       " 'hr1214-107': 'REFERRED',\n",
       " 'hr1215-107': 'REFERRED',\n",
       " 'hr1216-107': 'REFERRED',\n",
       " 'hr1217-107': 'REFERRED',\n",
       " 'hr1218-107': 'REFERRED',\n",
       " 'hr1219-107': 'REFERRED',\n",
       " 'hr122-107': 'REFERRED',\n",
       " 'hr1220-107': 'REFERRED',\n",
       " 'hr1221-107': 'REFERRED',\n",
       " 'hr1222-107': 'REFERRED',\n",
       " 'hr1223-107': 'REFERRED',\n",
       " 'hr1224-107': 'REFERRED',\n",
       " 'hr1225-107': 'REFERRED',\n",
       " 'hr1226-107': 'REFERRED',\n",
       " 'hr1227-107': 'REFERRED',\n",
       " 'hr1228-107': 'REFERRED',\n",
       " 'hr1229-107': 'REFERRED',\n",
       " 'hr123-107': 'REFERRED',\n",
       " 'hr1230-107': 'ENACTED:SIGNED',\n",
       " 'hr1231-107': 'REFERRED',\n",
       " 'hr1232-107': 'REFERRED',\n",
       " 'hr1233-107': 'REFERRED',\n",
       " 'hr1234-107': 'REFERRED',\n",
       " 'hr1235-107': 'REFERRED',\n",
       " 'hr1236-107': 'REFERRED',\n",
       " 'hr1237-107': 'REFERRED',\n",
       " 'hr1238-107': 'REFERRED',\n",
       " 'hr1239-107': 'REFERRED',\n",
       " 'hr124-107': 'REFERRED',\n",
       " 'hr1240-107': 'REFERRED',\n",
       " 'hr1241-107': 'REFERRED',\n",
       " 'hr1242-107': 'REFERRED',\n",
       " 'hr1243-107': 'REFERRED',\n",
       " 'hr1244-107': 'REFERRED',\n",
       " 'hr1245-107': 'REFERRED',\n",
       " 'hr1246-107': 'REFERRED',\n",
       " 'hr1247-107': 'REFERRED',\n",
       " 'hr1248-107': 'REFERRED',\n",
       " 'hr1249-107': 'REFERRED',\n",
       " 'hr125-107': 'REFERRED',\n",
       " 'hr1250-107': 'REFERRED',\n",
       " 'hr1251-107': 'REFERRED',\n",
       " 'hr1252-107': 'REFERRED',\n",
       " 'hr1253-107': 'REFERRED',\n",
       " 'hr1254-107': 'REFERRED',\n",
       " 'hr1255-107': 'REFERRED',\n",
       " 'hr1256-107': 'REFERRED',\n",
       " 'hr1257-107': 'REFERRED',\n",
       " 'hr1258-107': 'REFERRED',\n",
       " 'hr1259-107': 'PASS_OVER:HOUSE',\n",
       " 'hr126-107': 'REFERRED',\n",
       " 'hr1260-107': 'REFERRED',\n",
       " 'hr1261-107': 'REFERRED',\n",
       " 'hr1262-107': 'REFERRED',\n",
       " 'hr1263-107': 'REFERRED',\n",
       " 'hr1264-107': 'REFERRED',\n",
       " 'hr1265-107': 'REFERRED',\n",
       " 'hr1266-107': 'REFERRED',\n",
       " 'hr1267-107': 'REFERRED',\n",
       " 'hr1268-107': 'REFERRED',\n",
       " 'hr1269-107': 'REFERRED',\n",
       " 'hr127-107': 'REFERRED',\n",
       " 'hr1270-107': 'REFERRED',\n",
       " 'hr1271-107': 'REFERRED',\n",
       " 'hr1272-107': 'REFERRED',\n",
       " 'hr1273-107': 'REFERRED',\n",
       " 'hr1274-107': 'REFERRED',\n",
       " 'hr1275-107': 'REFERRED',\n",
       " 'hr1276-107': 'REFERRED',\n",
       " 'hr1277-107': 'REFERRED',\n",
       " 'hr1278-107': 'REFERRED',\n",
       " 'hr1279-107': 'REFERRED',\n",
       " 'hr128-107': 'REFERRED',\n",
       " 'hr1280-107': 'REFERRED',\n",
       " 'hr1281-107': 'REFERRED',\n",
       " 'hr1282-107': 'REFERRED',\n",
       " 'hr1283-107': 'REFERRED',\n",
       " 'hr1284-107': 'REFERRED',\n",
       " 'hr1285-107': 'REFERRED',\n",
       " 'hr1286-107': 'REFERRED',\n",
       " 'hr1287-107': 'REFERRED',\n",
       " 'hr1288-107': 'REFERRED',\n",
       " 'hr1289-107': 'REFERRED',\n",
       " 'hr129-107': 'REFERRED',\n",
       " 'hr1290-107': 'REFERRED',\n",
       " 'hr1291-107': 'ENACTED:SIGNED',\n",
       " 'hr1292-107': 'REFERRED',\n",
       " 'hr1293-107': 'REFERRED',\n",
       " 'hr1294-107': 'REFERRED',\n",
       " 'hr1295-107': 'REFERRED',\n",
       " 'hr1296-107': 'REFERRED',\n",
       " 'hr1297-107': 'REFERRED',\n",
       " 'hr1298-107': 'REFERRED',\n",
       " 'hr1299-107': 'REFERRED',\n",
       " 'hr13-107': 'REFERRED',\n",
       " 'hr130-107': 'REFERRED',\n",
       " 'hr1300-107': 'REFERRED',\n",
       " 'hr1301-107': 'REFERRED',\n",
       " 'hr1302-107': 'REFERRED',\n",
       " 'hr1303-107': 'REFERRED',\n",
       " 'hr1304-107': 'REFERRED',\n",
       " 'hr1305-107': 'REFERRED',\n",
       " 'hr1306-107': 'REFERRED',\n",
       " 'hr1307-107': 'REFERRED',\n",
       " 'hr1308-107': 'REFERRED',\n",
       " 'hr1309-107': 'REFERRED',\n",
       " 'hr131-107': 'REFERRED',\n",
       " 'hr1310-107': 'REFERRED',\n",
       " 'hr1311-107': 'REFERRED',\n",
       " 'hr1312-107': 'REFERRED',\n",
       " 'hr1313-107': 'REFERRED',\n",
       " 'hr1314-107': 'REFERRED',\n",
       " 'hr1315-107': 'REFERRED',\n",
       " 'hr1316-107': 'REFERRED',\n",
       " 'hr1317-107': 'REFERRED',\n",
       " 'hr1318-107': 'REFERRED',\n",
       " 'hr1319-107': 'REFERRED',\n",
       " 'hr132-107': 'ENACTED:SIGNED',\n",
       " 'hr1320-107': 'REFERRED',\n",
       " 'hr1321-107': 'REFERRED',\n",
       " 'hr1322-107': 'REFERRED',\n",
       " 'hr1323-107': 'REFERRED',\n",
       " 'hr1324-107': 'REFERRED',\n",
       " 'hr1325-107': 'REFERRED',\n",
       " 'hr1326-107': 'REFERRED',\n",
       " 'hr1327-107': 'REFERRED',\n",
       " 'hr1328-107': 'REFERRED',\n",
       " 'hr1329-107': 'REFERRED',\n",
       " 'hr133-107': 'REFERRED',\n",
       " 'hr1330-107': 'REFERRED',\n",
       " 'hr1331-107': 'REFERRED',\n",
       " 'hr1332-107': 'REFERRED',\n",
       " 'hr1333-107': 'REFERRED',\n",
       " 'hr1334-107': 'REFERRED',\n",
       " 'hr1335-107': 'REFERRED',\n",
       " 'hr1336-107': 'REFERRED',\n",
       " 'hr1337-107': 'REFERRED',\n",
       " 'hr1338-107': 'REFERRED',\n",
       " 'hr1339-107': 'REFERRED',\n",
       " 'hr134-107': 'REFERRED',\n",
       " 'hr1340-107': 'REFERRED',\n",
       " 'hr1341-107': 'REFERRED',\n",
       " 'hr1342-107': 'REFERRED',\n",
       " 'hr1343-107': 'REFERRED',\n",
       " 'hr1344-107': 'REFERRED',\n",
       " 'hr1345-107': 'REFERRED',\n",
       " 'hr1346-107': 'REFERRED',\n",
       " 'hr1347-107': 'REFERRED',\n",
       " 'hr1348-107': 'REFERRED',\n",
       " 'hr1349-107': 'REFERRED',\n",
       " 'hr135-107': 'REFERRED',\n",
       " 'hr1350-107': 'REFERRED',\n",
       " 'hr1351-107': 'REFERRED',\n",
       " 'hr1352-107': 'REFERRED',\n",
       " 'hr1353-107': 'REFERRED',\n",
       " 'hr1354-107': 'REFERRED',\n",
       " 'hr1355-107': 'REFERRED',\n",
       " 'hr1356-107': 'REFERRED',\n",
       " 'hr1357-107': 'REFERRED',\n",
       " 'hr1358-107': 'REFERRED',\n",
       " 'hr1359-107': 'REFERRED',\n",
       " 'hr136-107': 'REFERRED',\n",
       " 'hr1360-107': 'REFERRED',\n",
       " 'hr1361-107': 'REFERRED',\n",
       " 'hr1362-107': 'REFERRED',\n",
       " 'hr1363-107': 'REFERRED',\n",
       " 'hr1364-107': 'REFERRED',\n",
       " 'hr1365-107': 'REFERRED',\n",
       " 'hr1366-107': 'ENACTED:SIGNED',\n",
       " 'hr1367-107': 'REFERRED',\n",
       " 'hr1368-107': 'REFERRED',\n",
       " 'hr1369-107': 'REFERRED',\n",
       " 'hr137-107': 'REFERRED',\n",
       " 'hr1370-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1371-107': 'REFERRED',\n",
       " 'hr1372-107': 'REFERRED',\n",
       " 'hr1373-107': 'REFERRED',\n",
       " 'hr1374-107': 'ENACTED:SIGNED',\n",
       " 'hr1375-107': 'REFERRED',\n",
       " 'hr1376-107': 'REFERRED',\n",
       " 'hr1377-107': 'REFERRED',\n",
       " 'hr1378-107': 'REFERRED',\n",
       " 'hr1379-107': 'REFERRED',\n",
       " 'hr138-107': 'REFERRED',\n",
       " 'hr1380-107': 'REFERRED',\n",
       " 'hr1381-107': 'REFERRED',\n",
       " 'hr1382-107': 'REFERRED',\n",
       " 'hr1383-107': 'REFERRED',\n",
       " 'hr1384-107': 'ENACTED:SIGNED',\n",
       " 'hr1385-107': 'REFERRED',\n",
       " 'hr1386-107': 'REFERRED',\n",
       " 'hr1387-107': 'REFERRED',\n",
       " 'hr1388-107': 'REFERRED',\n",
       " 'hr1389-107': 'REFERRED',\n",
       " 'hr139-107': 'REFERRED',\n",
       " 'hr1390-107': 'REFERRED',\n",
       " 'hr1391-107': 'REFERRED',\n",
       " 'hr1392-107': 'REFERRED',\n",
       " 'hr1393-107': 'REFERRED',\n",
       " 'hr1394-107': 'REFERRED',\n",
       " 'hr1395-107': 'REFERRED',\n",
       " 'hr1396-107': 'REFERRED',\n",
       " 'hr1397-107': 'REFERRED',\n",
       " 'hr1398-107': 'REFERRED',\n",
       " 'hr1399-107': 'REFERRED',\n",
       " 'hr14-107': 'REFERRED',\n",
       " 'hr140-107': 'REFERRED',\n",
       " 'hr1400-107': 'REFERRED',\n",
       " 'hr1401-107': 'REFERRED',\n",
       " 'hr1402-107': 'REFERRED',\n",
       " 'hr1403-107': 'REFERRED',\n",
       " 'hr1404-107': 'REFERRED',\n",
       " 'hr1405-107': 'REFERRED',\n",
       " 'hr1406-107': 'REFERRED',\n",
       " 'hr1407-107': 'REPORTED',\n",
       " 'hr1408-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1409-107': 'REFERRED',\n",
       " 'hr141-107': 'REFERRED',\n",
       " 'hr1410-107': 'REFERRED',\n",
       " 'hr1411-107': 'REFERRED',\n",
       " 'hr1412-107': 'REFERRED',\n",
       " 'hr1413-107': 'REFERRED',\n",
       " 'hr1414-107': 'REFERRED',\n",
       " 'hr1415-107': 'REFERRED',\n",
       " 'hr1416-107': 'REFERRED',\n",
       " 'hr1417-107': 'REFERRED',\n",
       " 'hr1418-107': 'REFERRED',\n",
       " 'hr1419-107': 'REFERRED',\n",
       " 'hr142-107': 'REFERRED',\n",
       " 'hr1420-107': 'REFERRED',\n",
       " 'hr1421-107': 'REFERRED',\n",
       " 'hr1422-107': 'REFERRED',\n",
       " 'hr1423-107': 'REFERRED',\n",
       " 'hr1424-107': 'REFERRED',\n",
       " 'hr1425-107': 'REFERRED',\n",
       " 'hr1426-107': 'REFERRED',\n",
       " 'hr1427-107': 'REFERRED',\n",
       " 'hr1428-107': 'REFERRED',\n",
       " 'hr1429-107': 'REFERRED',\n",
       " 'hr143-107': 'REFERRED',\n",
       " 'hr1430-107': 'REFERRED',\n",
       " 'hr1431-107': 'REFERRED',\n",
       " 'hr1432-107': 'ENACTED:SIGNED',\n",
       " 'hr1433-107': 'REFERRED',\n",
       " 'hr1434-107': 'REFERRED',\n",
       " 'hr1435-107': 'REFERRED',\n",
       " 'hr1436-107': 'REFERRED',\n",
       " 'hr1437-107': 'REFERRED',\n",
       " 'hr1438-107': 'REFERRED',\n",
       " 'hr1439-107': 'REFERRED',\n",
       " 'hr144-107': 'REFERRED',\n",
       " 'hr1440-107': 'REFERRED',\n",
       " 'hr1441-107': 'REFERRED',\n",
       " 'hr1442-107': 'REFERRED',\n",
       " 'hr1443-107': 'REFERRED',\n",
       " 'hr1444-107': 'REFERRED',\n",
       " 'hr1445-107': 'REFERRED',\n",
       " 'hr1446-107': 'REFERRED',\n",
       " 'hr1447-107': 'REFERRED',\n",
       " 'hr1448-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1449-107': 'REFERRED',\n",
       " 'hr145-107': 'REFERRED',\n",
       " 'hr1450-107': 'REFERRED',\n",
       " 'hr1451-107': 'REFERRED',\n",
       " 'hr1452-107': 'REPORTED',\n",
       " 'hr1453-107': 'REFERRED',\n",
       " 'hr1454-107': 'REFERRED',\n",
       " 'hr1455-107': 'REFERRED',\n",
       " 'hr1456-107': 'ENACTED:SIGNED',\n",
       " 'hr1457-107': 'REFERRED',\n",
       " 'hr1458-107': 'REFERRED',\n",
       " 'hr1459-107': 'REFERRED',\n",
       " 'hr146-107': 'ENACTED:SIGNED',\n",
       " 'hr1460-107': 'REFERRED',\n",
       " 'hr1461-107': 'REFERRED',\n",
       " 'hr1462-107': 'REPORTED',\n",
       " 'hr1463-107': 'REFERRED',\n",
       " 'hr1464-107': 'REFERRED',\n",
       " 'hr1465-107': 'REFERRED',\n",
       " 'hr1466-107': 'REFERRED',\n",
       " 'hr1467-107': 'REFERRED',\n",
       " 'hr1468-107': 'REFERRED',\n",
       " 'hr1469-107': 'REFERRED',\n",
       " 'hr147-107': 'REFERRED',\n",
       " 'hr1470-107': 'REFERRED',\n",
       " 'hr1471-107': 'REFERRED',\n",
       " 'hr1472-107': 'REFERRED',\n",
       " 'hr1473-107': 'REFERRED',\n",
       " 'hr1474-107': 'REFERRED',\n",
       " 'hr1475-107': 'REFERRED',\n",
       " 'hr1476-107': 'REFERRED',\n",
       " 'hr1477-107': 'REFERRED',\n",
       " 'hr1478-107': 'REFERRED',\n",
       " 'hr1479-107': 'REFERRED',\n",
       " 'hr148-107': 'REFERRED',\n",
       " 'hr1480-107': 'REFERRED',\n",
       " 'hr1481-107': 'REFERRED',\n",
       " 'hr1482-107': 'REFERRED',\n",
       " 'hr1483-107': 'REFERRED',\n",
       " 'hr1484-107': 'REFERRED',\n",
       " 'hr1485-107': 'REFERRED',\n",
       " 'hr1486-107': 'REFERRED',\n",
       " 'hr1487-107': 'REFERRED',\n",
       " 'hr1488-107': 'REFERRED',\n",
       " 'hr1489-107': 'REFERRED',\n",
       " 'hr149-107': 'REFERRED',\n",
       " 'hr1490-107': 'REFERRED',\n",
       " 'hr1491-107': 'REPORTED',\n",
       " 'hr1492-107': 'REFERRED',\n",
       " 'hr1493-107': 'REFERRED',\n",
       " 'hr1494-107': 'REFERRED',\n",
       " 'hr1495-107': 'REFERRED',\n",
       " 'hr1496-107': 'REFERRED',\n",
       " 'hr1497-107': 'REFERRED',\n",
       " 'hr1498-107': 'REFERRED',\n",
       " 'hr1499-107': 'ENACTED:SIGNED',\n",
       " 'hr15-107': 'REFERRED',\n",
       " 'hr150-107': 'REFERRED',\n",
       " 'hr1500-107': 'REFERRED',\n",
       " 'hr1501-107': 'REFERRED',\n",
       " 'hr1502-107': 'REFERRED',\n",
       " 'hr1503-107': 'REFERRED',\n",
       " 'hr1504-107': 'REFERRED',\n",
       " 'hr1505-107': 'REFERRED',\n",
       " 'hr1506-107': 'REFERRED',\n",
       " 'hr1507-107': 'REFERRED',\n",
       " 'hr1508-107': 'REFERRED',\n",
       " 'hr1509-107': 'REFERRED',\n",
       " 'hr151-107': 'REFERRED',\n",
       " 'hr1510-107': 'REFERRED',\n",
       " 'hr1511-107': 'REFERRED',\n",
       " 'hr1512-107': 'REFERRED',\n",
       " 'hr1513-107': 'REFERRED',\n",
       " 'hr1514-107': 'REFERRED',\n",
       " 'hr1515-107': 'REFERRED',\n",
       " 'hr1516-107': 'REFERRED',\n",
       " 'hr1517-107': 'REFERRED',\n",
       " 'hr1518-107': 'REFERRED',\n",
       " 'hr1519-107': 'REFERRED',\n",
       " 'hr152-107': 'REFERRED',\n",
       " 'hr1520-107': 'REFERRED',\n",
       " 'hr1521-107': 'REFERRED',\n",
       " 'hr1522-107': 'REFERRED',\n",
       " 'hr1523-107': 'REFERRED',\n",
       " 'hr1524-107': 'REFERRED',\n",
       " 'hr1525-107': 'REFERRED',\n",
       " 'hr1526-107': 'REFERRED',\n",
       " 'hr1527-107': 'REFERRED',\n",
       " 'hr1528-107': 'REFERRED',\n",
       " 'hr1529-107': 'REFERRED',\n",
       " 'hr153-107': 'REFERRED',\n",
       " 'hr1530-107': 'REFERRED',\n",
       " 'hr1531-107': 'REFERRED',\n",
       " 'hr1532-107': 'REFERRED',\n",
       " 'hr1533-107': 'REFERRED',\n",
       " 'hr1534-107': 'REFERRED',\n",
       " 'hr1535-107': 'REFERRED',\n",
       " 'hr1536-107': 'REFERRED',\n",
       " 'hr1537-107': 'REFERRED',\n",
       " 'hr1538-107': 'REFERRED',\n",
       " 'hr1539-107': 'REFERRED',\n",
       " 'hr154-107': 'REFERRED',\n",
       " 'hr1540-107': 'REFERRED',\n",
       " 'hr1541-107': 'REFERRED',\n",
       " 'hr1542-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1543-107': 'REFERRED',\n",
       " 'hr1544-107': 'REFERRED',\n",
       " 'hr1545-107': 'REFERRED',\n",
       " 'hr1546-107': 'REFERRED',\n",
       " 'hr1547-107': 'REFERRED',\n",
       " 'hr1548-107': 'REFERRED',\n",
       " 'hr1549-107': 'REFERRED',\n",
       " 'hr155-107': 'REFERRED',\n",
       " 'hr1550-107': 'REFERRED',\n",
       " 'hr1551-107': 'REFERRED',\n",
       " 'hr1552-107': 'ENACTED:SIGNED',\n",
       " 'hr1553-107': 'REFERRED',\n",
       " 'hr1554-107': 'REFERRED',\n",
       " 'hr1555-107': 'REFERRED',\n",
       " 'hr1556-107': 'REFERRED',\n",
       " 'hr1557-107': 'REFERRED',\n",
       " 'hr1558-107': 'REFERRED',\n",
       " 'hr1559-107': 'REFERRED',\n",
       " 'hr156-107': 'REFERRED',\n",
       " 'hr1560-107': 'REFERRED',\n",
       " 'hr1561-107': 'REFERRED',\n",
       " 'hr1562-107': 'REFERRED',\n",
       " 'hr1563-107': 'REFERRED',\n",
       " 'hr1564-107': 'REFERRED',\n",
       " 'hr1565-107': 'REFERRED',\n",
       " 'hr1566-107': 'REFERRED',\n",
       " 'hr1567-107': 'REFERRED',\n",
       " 'hr1568-107': 'REFERRED',\n",
       " 'hr1569-107': 'REFERRED',\n",
       " 'hr157-107': 'REFERRED',\n",
       " 'hr1570-107': 'REFERRED',\n",
       " 'hr1571-107': 'REFERRED',\n",
       " 'hr1572-107': 'REFERRED',\n",
       " 'hr1573-107': 'REFERRED',\n",
       " 'hr1574-107': 'REFERRED',\n",
       " 'hr1575-107': 'REFERRED',\n",
       " 'hr1576-107': 'ENACTED:SIGNED',\n",
       " 'hr1577-107': 'REPORTED',\n",
       " 'hr1578-107': 'REFERRED',\n",
       " 'hr1579-107': 'REFERRED',\n",
       " 'hr158-107': 'REFERRED',\n",
       " 'hr1580-107': 'REFERRED',\n",
       " 'hr1581-107': 'REFERRED',\n",
       " 'hr1582-107': 'REFERRED',\n",
       " 'hr1583-107': 'ENACTED:SIGNED',\n",
       " 'hr1584-107': 'REFERRED',\n",
       " 'hr1585-107': 'REFERRED',\n",
       " 'hr1586-107': 'REFERRED',\n",
       " 'hr1587-107': 'REFERRED',\n",
       " 'hr1588-107': 'REFERRED',\n",
       " 'hr1589-107': 'REFERRED',\n",
       " 'hr159-107': 'REFERRED',\n",
       " 'hr1590-107': 'REFERRED',\n",
       " 'hr1591-107': 'REFERRED',\n",
       " 'hr1592-107': 'REFERRED',\n",
       " 'hr1593-107': 'REFERRED',\n",
       " 'hr1594-107': 'REFERRED',\n",
       " 'hr1595-107': 'REFERRED',\n",
       " 'hr1596-107': 'REFERRED',\n",
       " 'hr1597-107': 'REFERRED',\n",
       " 'hr1598-107': 'REFERRED',\n",
       " 'hr1599-107': 'REFERRED',\n",
       " 'hr16-107': 'REFERRED',\n",
       " 'hr160-107': 'REFERRED',\n",
       " 'hr1600-107': 'REFERRED',\n",
       " 'hr1601-107': 'REFERRED',\n",
       " 'hr1602-107': 'REFERRED',\n",
       " 'hr1603-107': 'REFERRED',\n",
       " 'hr1604-107': 'REFERRED',\n",
       " 'hr1605-107': 'REFERRED',\n",
       " 'hr1606-107': 'PASS_BACK:SENATE',\n",
       " 'hr1607-107': 'REFERRED',\n",
       " 'hr1608-107': 'REFERRED',\n",
       " 'hr1609-107': 'REFERRED',\n",
       " 'hr161-107': 'REFERRED',\n",
       " 'hr1610-107': 'REFERRED',\n",
       " 'hr1611-107': 'REFERRED',\n",
       " 'hr1612-107': 'REFERRED',\n",
       " 'hr1613-107': 'REFERRED',\n",
       " 'hr1614-107': 'REFERRED',\n",
       " 'hr1615-107': 'REFERRED',\n",
       " 'hr1616-107': 'REFERRED',\n",
       " 'hr1617-107': 'REFERRED',\n",
       " 'hr1618-107': 'REFERRED',\n",
       " 'hr1619-107': 'REPORTED',\n",
       " 'hr162-107': 'REFERRED',\n",
       " 'hr1620-107': 'REFERRED',\n",
       " 'hr1621-107': 'REFERRED',\n",
       " 'hr1622-107': 'REFERRED',\n",
       " 'hr1623-107': 'REFERRED',\n",
       " 'hr1624-107': 'REFERRED',\n",
       " 'hr1625-107': 'REFERRED',\n",
       " 'hr1626-107': 'REFERRED',\n",
       " 'hr1627-107': 'REFERRED',\n",
       " 'hr1628-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1629-107': 'REFERRED',\n",
       " 'hr163-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1630-107': 'REFERRED',\n",
       " 'hr1631-107': 'REFERRED',\n",
       " 'hr1632-107': 'REFERRED',\n",
       " 'hr1633-107': 'REFERRED',\n",
       " 'hr1634-107': 'REFERRED',\n",
       " 'hr1635-107': 'REFERRED',\n",
       " 'hr1636-107': 'REFERRED',\n",
       " 'hr1637-107': 'REFERRED',\n",
       " 'hr1638-107': 'REFERRED',\n",
       " 'hr1639-107': 'REFERRED',\n",
       " 'hr164-107': 'REFERRED',\n",
       " 'hr1640-107': 'REFERRED',\n",
       " 'hr1641-107': 'REFERRED',\n",
       " 'hr1642-107': 'REFERRED',\n",
       " 'hr1643-107': 'REFERRED',\n",
       " 'hr1644-107': 'REFERRED',\n",
       " 'hr1645-107': 'REFERRED',\n",
       " 'hr1646-107': 'ENACTED:SIGNED',\n",
       " 'hr1647-107': 'REFERRED',\n",
       " 'hr1648-107': 'REFERRED',\n",
       " 'hr1649-107': 'REFERRED',\n",
       " 'hr165-107': 'REFERRED',\n",
       " 'hr1650-107': 'REFERRED',\n",
       " 'hr1651-107': 'REFERRED',\n",
       " 'hr1652-107': 'REFERRED',\n",
       " 'hr1653-107': 'REFERRED',\n",
       " 'hr1654-107': 'REFERRED',\n",
       " 'hr1655-107': 'REFERRED',\n",
       " 'hr1656-107': 'REFERRED',\n",
       " 'hr1657-107': 'REFERRED',\n",
       " 'hr1658-107': 'REFERRED',\n",
       " 'hr1659-107': 'REFERRED',\n",
       " 'hr166-107': 'REFERRED',\n",
       " 'hr1660-107': 'REFERRED',\n",
       " 'hr1661-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1662-107': 'REFERRED',\n",
       " 'hr1663-107': 'REFERRED',\n",
       " 'hr1664-107': 'REFERRED',\n",
       " 'hr1665-107': 'REFERRED',\n",
       " 'hr1666-107': 'REFERRED',\n",
       " 'hr1667-107': 'REFERRED',\n",
       " 'hr1668-107': 'ENACTED:SIGNED',\n",
       " 'hr1669-107': 'REFERRED',\n",
       " 'hr167-107': 'REFERRED',\n",
       " 'hr1670-107': 'REFERRED',\n",
       " 'hr1671-107': 'REFERRED',\n",
       " 'hr1672-107': 'REFERRED',\n",
       " 'hr1673-107': 'REFERRED',\n",
       " 'hr1674-107': 'REFERRED',\n",
       " 'hr1675-107': 'REFERRED',\n",
       " 'hr1676-107': 'REFERRED',\n",
       " 'hr1677-107': 'REFERRED',\n",
       " 'hr1678-107': 'REFERRED',\n",
       " 'hr1679-107': 'REFERRED',\n",
       " 'hr168-107': 'REFERRED',\n",
       " 'hr1680-107': 'REFERRED',\n",
       " 'hr1681-107': 'REFERRED',\n",
       " 'hr1682-107': 'REFERRED',\n",
       " 'hr1683-107': 'REFERRED',\n",
       " 'hr1684-107': 'REFERRED',\n",
       " 'hr1685-107': 'REFERRED',\n",
       " 'hr1686-107': 'REFERRED',\n",
       " 'hr1687-107': 'REFERRED',\n",
       " 'hr1688-107': 'REFERRED',\n",
       " 'hr1689-107': 'REFERRED',\n",
       " 'hr169-107': 'ENACTED:SIGNED',\n",
       " 'hr1690-107': 'REFERRED',\n",
       " 'hr1691-107': 'REFERRED',\n",
       " 'hr1692-107': 'REFERRED',\n",
       " 'hr1693-107': 'REFERRED',\n",
       " 'hr1694-107': 'REFERRED',\n",
       " 'hr1695-107': 'REFERRED',\n",
       " 'hr1696-107': 'ENACTED:SIGNED',\n",
       " 'hr1697-107': 'REFERRED',\n",
       " 'hr1698-107': 'REFERRED',\n",
       " 'hr1699-107': 'PASS_OVER:HOUSE',\n",
       " 'hr17-107': 'REFERRED',\n",
       " 'hr170-107': 'REFERRED',\n",
       " 'hr1700-107': 'REFERRED',\n",
       " 'hr1701-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1702-107': 'REFERRED',\n",
       " 'hr1703-107': 'REFERRED',\n",
       " 'hr1704-107': 'REFERRED',\n",
       " 'hr1705-107': 'REFERRED',\n",
       " 'hr1706-107': 'REFERRED',\n",
       " 'hr1707-107': 'REFERRED',\n",
       " 'hr1708-107': 'REFERRED',\n",
       " 'hr1709-107': 'REFERRED',\n",
       " 'hr171-107': 'REFERRED',\n",
       " 'hr1710-107': 'REFERRED',\n",
       " 'hr1711-107': 'REFERRED',\n",
       " 'hr1712-107': 'ENACTED:SIGNED',\n",
       " 'hr1713-107': 'REFERRED',\n",
       " 'hr1714-107': 'REFERRED',\n",
       " 'hr1715-107': 'REFERRED',\n",
       " 'hr1716-107': 'REFERRED',\n",
       " 'hr1717-107': 'REFERRED',\n",
       " 'hr1718-107': 'REFERRED',\n",
       " 'hr1719-107': 'REFERRED',\n",
       " 'hr172-107': 'REFERRED',\n",
       " 'hr1720-107': 'REFERRED',\n",
       " 'hr1721-107': 'REFERRED',\n",
       " 'hr1722-107': 'REFERRED',\n",
       " 'hr1723-107': 'REFERRED',\n",
       " 'hr1724-107': 'REFERRED',\n",
       " 'hr1725-107': 'REFERRED',\n",
       " 'hr1726-107': 'REFERRED',\n",
       " 'hr1727-107': 'ENACTED:SIGNED',\n",
       " 'hr1728-107': 'REFERRED',\n",
       " 'hr1729-107': 'REFERRED',\n",
       " 'hr173-107': 'REFERRED',\n",
       " 'hr1730-107': 'REFERRED',\n",
       " 'hr1731-107': 'REFERRED',\n",
       " 'hr1732-107': 'REFERRED',\n",
       " 'hr1733-107': 'REFERRED',\n",
       " 'hr1734-107': 'REFERRED',\n",
       " 'hr1735-107': 'REFERRED',\n",
       " 'hr1736-107': 'REFERRED',\n",
       " 'hr1737-107': 'REFERRED',\n",
       " 'hr1738-107': 'REFERRED',\n",
       " 'hr1739-107': 'REFERRED',\n",
       " 'hr174-107': 'REFERRED',\n",
       " 'hr1740-107': 'REFERRED',\n",
       " 'hr1741-107': 'REFERRED',\n",
       " 'hr1742-107': 'REFERRED',\n",
       " 'hr1743-107': 'REFERRED',\n",
       " 'hr1744-107': 'REFERRED',\n",
       " 'hr1745-107': 'REFERRED',\n",
       " 'hr1746-107': 'REFERRED',\n",
       " 'hr1747-107': 'REFERRED',\n",
       " 'hr1748-107': 'ENACTED:SIGNED',\n",
       " 'hr1749-107': 'ENACTED:SIGNED',\n",
       " 'hr175-107': 'REFERRED',\n",
       " 'hr1750-107': 'REFERRED',\n",
       " 'hr1751-107': 'REFERRED',\n",
       " 'hr1752-107': 'REFERRED',\n",
       " 'hr1753-107': 'ENACTED:SIGNED',\n",
       " 'hr1754-107': 'REFERRED',\n",
       " 'hr1755-107': 'REFERRED',\n",
       " 'hr1756-107': 'REFERRED',\n",
       " 'hr1757-107': 'REFERRED',\n",
       " 'hr1758-107': 'REFERRED',\n",
       " 'hr1759-107': 'REFERRED',\n",
       " 'hr176-107': 'REFERRED',\n",
       " 'hr1760-107': 'REFERRED',\n",
       " 'hr1761-107': 'ENACTED:SIGNED',\n",
       " 'hr1762-107': 'REFERRED',\n",
       " 'hr1763-107': 'REFERRED',\n",
       " 'hr1764-107': 'REFERRED',\n",
       " 'hr1765-107': 'REFERRED',\n",
       " 'hr1766-107': 'ENACTED:SIGNED',\n",
       " 'hr1767-107': 'REFERRED',\n",
       " 'hr1768-107': 'REFERRED',\n",
       " 'hr1769-107': 'REFERRED',\n",
       " 'hr177-107': 'REFERRED',\n",
       " 'hr1770-107': 'REFERRED',\n",
       " 'hr1771-107': 'REFERRED',\n",
       " 'hr1772-107': 'REFERRED',\n",
       " 'hr1773-107': 'REFERRED',\n",
       " 'hr1774-107': 'REFERRED',\n",
       " 'hr1775-107': 'REFERRED',\n",
       " 'hr1776-107': 'ENACTED:SIGNED',\n",
       " 'hr1777-107': 'REFERRED',\n",
       " 'hr1778-107': 'REFERRED',\n",
       " 'hr1779-107': 'REFERRED',\n",
       " 'hr178-107': 'REFERRED',\n",
       " 'hr1780-107': 'REFERRED',\n",
       " 'hr1781-107': 'REFERRED',\n",
       " 'hr1782-107': 'REFERRED',\n",
       " 'hr1783-107': 'REFERRED',\n",
       " 'hr1784-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1785-107': 'REFERRED',\n",
       " 'hr1786-107': 'REFERRED',\n",
       " 'hr1787-107': 'REFERRED',\n",
       " 'hr1788-107': 'REFERRED',\n",
       " 'hr1789-107': 'REFERRED',\n",
       " 'hr179-107': 'REFERRED',\n",
       " 'hr1790-107': 'REFERRED',\n",
       " 'hr1791-107': 'REFERRED',\n",
       " 'hr1792-107': 'REFERRED',\n",
       " 'hr1793-107': 'REFERRED',\n",
       " 'hr1794-107': 'REFERRED',\n",
       " 'hr1795-107': 'REFERRED',\n",
       " 'hr1796-107': 'REFERRED',\n",
       " 'hr1797-107': 'REFERRED',\n",
       " 'hr1798-107': 'REFERRED',\n",
       " 'hr1799-107': 'REFERRED',\n",
       " 'hr18-107': 'REFERRED',\n",
       " 'hr180-107': 'REFERRED',\n",
       " 'hr1800-107': 'REFERRED',\n",
       " 'hr1801-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1802-107': 'REFERRED',\n",
       " 'hr1803-107': 'REFERRED',\n",
       " 'hr1804-107': 'REFERRED',\n",
       " 'hr1805-107': 'REFERRED',\n",
       " 'hr1806-107': 'REFERRED',\n",
       " 'hr1807-107': 'REFERRED',\n",
       " 'hr1808-107': 'REFERRED',\n",
       " 'hr1809-107': 'REFERRED',\n",
       " 'hr181-107': 'REFERRED',\n",
       " 'hr1810-107': 'REFERRED',\n",
       " 'hr1811-107': 'REPORTED',\n",
       " 'hr1812-107': 'REFERRED',\n",
       " 'hr1813-107': 'REFERRED',\n",
       " 'hr1814-107': 'ENACTED:SIGNED',\n",
       " 'hr1815-107': 'REFERRED',\n",
       " 'hr1816-107': 'REFERRED',\n",
       " 'hr1817-107': 'REFERRED',\n",
       " 'hr1818-107': 'REFERRED',\n",
       " 'hr1819-107': 'REFERRED',\n",
       " 'hr182-107': 'ENACTED:SIGNED',\n",
       " 'hr1820-107': 'REFERRED',\n",
       " 'hr1821-107': 'REFERRED',\n",
       " 'hr1822-107': 'REFERRED',\n",
       " 'hr1823-107': 'REFERRED',\n",
       " 'hr1824-107': 'REFERRED',\n",
       " 'hr1825-107': 'REFERRED',\n",
       " 'hr1826-107': 'REFERRED',\n",
       " 'hr1827-107': 'REFERRED',\n",
       " 'hr1828-107': 'REFERRED',\n",
       " 'hr1829-107': 'REFERRED',\n",
       " 'hr183-107': 'REFERRED',\n",
       " 'hr1830-107': 'REFERRED',\n",
       " 'hr1831-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1832-107': 'REFERRED',\n",
       " 'hr1833-107': 'REFERRED',\n",
       " 'hr1834-107': 'REFERRED',\n",
       " 'hr1835-107': 'REFERRED',\n",
       " 'hr1836-107': 'ENACTED:SIGNED',\n",
       " 'hr1837-107': 'REFERRED',\n",
       " 'hr1838-107': 'REFERRED',\n",
       " 'hr1839-107': 'REFERRED',\n",
       " 'hr184-107': 'REFERRED',\n",
       " 'hr1840-107': 'ENACTED:SIGNED',\n",
       " 'hr1841-107': 'REFERRED',\n",
       " 'hr1842-107': 'REFERRED',\n",
       " 'hr1843-107': 'REFERRED',\n",
       " 'hr1844-107': 'REFERRED',\n",
       " 'hr1845-107': 'REFERRED',\n",
       " 'hr1846-107': 'REFERRED',\n",
       " 'hr1847-107': 'REFERRED',\n",
       " 'hr1848-107': 'REFERRED',\n",
       " 'hr1849-107': 'REFERRED',\n",
       " 'hr185-107': 'REFERRED',\n",
       " 'hr1850-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1851-107': 'REFERRED',\n",
       " 'hr1852-107': 'REFERRED',\n",
       " 'hr1853-107': 'REFERRED',\n",
       " 'hr1854-107': 'REFERRED',\n",
       " 'hr1855-107': 'REFERRED',\n",
       " 'hr1856-107': 'REFERRED',\n",
       " 'hr1857-107': 'REFERRED',\n",
       " 'hr1858-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1859-107': 'REFERRED',\n",
       " 'hr186-107': 'REFERRED',\n",
       " 'hr1860-107': 'ENACTED:SIGNED',\n",
       " 'hr1861-107': 'REFERRED',\n",
       " 'hr1862-107': 'REFERRED',\n",
       " 'hr1863-107': 'REFERRED',\n",
       " 'hr1864-107': 'REFERRED',\n",
       " 'hr1865-107': 'REFERRED',\n",
       " 'hr1866-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1867-107': 'REFERRED',\n",
       " 'hr1868-107': 'REFERRED',\n",
       " 'hr1869-107': 'REFERRED',\n",
       " 'hr187-107': 'REFERRED',\n",
       " 'hr1870-107': 'ENACTED:SIGNED',\n",
       " 'hr1871-107': 'REFERRED',\n",
       " 'hr1872-107': 'REFERRED',\n",
       " 'hr1873-107': 'REFERRED',\n",
       " 'hr1874-107': 'REFERRED',\n",
       " 'hr1875-107': 'REFERRED',\n",
       " 'hr1876-107': 'REFERRED',\n",
       " 'hr1877-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1878-107': 'REFERRED',\n",
       " 'hr1879-107': 'REFERRED',\n",
       " 'hr188-107': 'REFERRED',\n",
       " 'hr1880-107': 'REFERRED',\n",
       " 'hr1881-107': 'REFERRED',\n",
       " 'hr1882-107': 'REFERRED',\n",
       " 'hr1883-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1884-107': 'REFERRED',\n",
       " 'hr1885-107': 'PASS_BACK:SENATE',\n",
       " 'hr1886-107': 'PASS_OVER:HOUSE',\n",
       " 'hr1887-107': 'REFERRED',\n",
       " 'hr1888-107': 'REFERRED',\n",
       " 'hr1889-107': 'REFERRED',\n",
       " 'hr189-107': 'REFERRED',\n",
       " 'hr1890-107': 'REFERRED',\n",
       " 'hr1891-107': 'REFERRED',\n",
       " 'hr1892-107': 'ENACTED:SIGNED',\n",
       " 'hr1893-107': 'REFERRED',\n",
       " 'hr1894-107': 'REFERRED',\n",
       " 'hr1895-107': 'REFERRED',\n",
       " 'hr1896-107': 'REFERRED',\n",
       " 'hr1897-107': 'REFERRED',\n",
       " 'hr1898-107': 'REFERRED',\n",
       " 'hr1899-107': 'REFERRED',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from status_dict\n",
    "status_df = pd.DataFrame.from_dict(status_dict,orient='index',columns=['Bill Status']).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Bill Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr1-107</td>\n",
       "      <td>ENACTED:SIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr10-107</td>\n",
       "      <td>ENACTED:SIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hr100-107</td>\n",
       "      <td>PASS_OVER:HOUSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hr1000-107</td>\n",
       "      <td>ENACTED:SIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hr1001-107</td>\n",
       "      <td>REFERRED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      Bill Status\n",
       "0     hr1-107   ENACTED:SIGNED\n",
       "1    hr10-107   ENACTED:SIGNED\n",
       "2   hr100-107  PASS_OVER:HOUSE\n",
       "3  hr1000-107   ENACTED:SIGNED\n",
       "4  hr1001-107         REFERRED"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map status values to binary\n",
    "\n",
    "#Once a Congress adjourn at the end of its two-year cycle, all bills that have been introduced in either\n",
    "#the House or the Senate that have not made it through the entire legislative process and signed into law are dead.\n",
    "\n",
    "#dictionary which specifies status value to binary\n",
    "#0 = did not pass in originating chamber (firs pass)\n",
    "#1 = did pass in originating chamber (first pass)\n",
    "#survive committee in the originating chamber (doing this for more class balance)\n",
    "\n",
    "status_binary_dict = {\n",
    "    'INTRODUCED':0,\n",
    "    'REFERRED':0,\n",
    "    'REPORTED':1,\n",
    "    'PROV_KILL:SUSPENSIONFAILED':1,\n",
    "    'PROV_KILL:CLOTUREFAILED':1,\n",
    "    'FAIL:ORIGINATING:HOUSE':1,\n",
    "    'FAIL:ORIGINATING:SENATE':1,\n",
    "    'PASSED:SIMPLERES':1,\n",
    "    'PASSED:CONSTAMEND':1,\n",
    "    'PASS_OVER:HOUSE':1,\n",
    "    'PASS_OVER:SENATE':1,\n",
    "    'PASSED:CONCURRENTRES':1,\n",
    "    'FAIL:SECOND:HOUSE':1,\n",
    "    'FAIL:SECOND:SENATE':1,\n",
    "    'PASS_BACK:HOUSE':1,\n",
    "    'PASS_BACK:SENATE':1,\n",
    "    'PROV_KILL:PINGPONGFAIL':1,\n",
    "    'PASSED:BILL':1,\n",
    "    'CONFERENCE:PASSED:HOUSE':1,\n",
    "    'CONFERENCE:PASSED:SENATE':1,\n",
    "    'ENACTED:SIGNED':1,\n",
    "    'PROV_KILL:VETO':1,\n",
    "    'VETOED:POCKET':1,\n",
    "    'VETOED:OVERRIDE_FAIL_ORIGINATING:HOUSE':1,\n",
    "    'VETOED:OVERRIDE_FAIL_ORIGINATING:SENATE':1,\n",
    "    'VETOED:OVERRIDE_PASS_OVER:HOUSE':1,\n",
    "    'VETOED:OVERRIDE_PASS_OVER:SENATE':1,\n",
    "    'VETOED:OVERRIDE_FAIL_SECOND:HOUSE':1,\n",
    "    'VETOED:OVERRIDE_FAIL_SECOND:SENATE':1,\n",
    "    'ENACTED:VETO_OVERRIDE':1,\n",
    "    'ENACTED:TENDAYRULE':1,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use status_binary_dict to map values in dataframe\n",
    "status_df[\"Bill Status\"].replace(status_binary_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Bill Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr1-107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr10-107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hr100-107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hr1000-107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hr1001-107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  Bill Status\n",
       "0     hr1-107            1\n",
       "1    hr10-107            1\n",
       "2   hr100-107            1\n",
       "3  hr1000-107            1\n",
       "4  hr1001-107            0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79089, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df['Bill Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11395"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df['Bill Status'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load bill text data\n",
    "\n",
    "with open('..\\\\Data\\\\107th-112th Congress\\\\Bill text\\\\HR_text_to_114.json') as f:\n",
    "    HR_data= json.load(f)\n",
    "    \n",
    "with open('..\\\\Data\\\\107th-112th Congress\\\\Bill text\\\\Sen_text_to_114.json') as f:\n",
    "    Sen_data= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dicts into one\n",
    "HR_data.update(Sen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77565"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HR_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "def clean_text(text):\n",
    "    #Remove underscores\n",
    "    text = re.sub('\\_','',text)\n",
    "    return text\n",
    "\n",
    "#Remove extended ellipses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bill_text= {k:clean_text(v) for k,v in HR_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read clustered data to dataframe\n",
    "text_df = pd.DataFrame.from_dict(clean_bill_text,orient='index', columns=['Text'])\n",
    "text_df.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107hr1ih</td>\n",
       "      <td>a bill to close the achievement gap with acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107hr10ih</td>\n",
       "      <td>to provide for pension reform, and for other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107hr100ih</td>\n",
       "      <td>to establish and expand programs relating to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107hr1000ih</td>\n",
       "      <td>to adjust the boundary of the william howard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107hr1001ih</td>\n",
       "      <td>to amend title xix of the social security act...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                               Text\n",
       "0     107hr1ih   a bill to close the achievement gap with acco...\n",
       "1    107hr10ih   to provide for pension reform, and for other ...\n",
       "2   107hr100ih   to establish and expand programs relating to ...\n",
       "3  107hr1000ih   to adjust the boundary of the william howard ...\n",
       "4  107hr1001ih   to amend title xix of the social security act..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_id(i):\n",
    "    j=i[:-2]\n",
    "    k = j[3:]+'-'+j[:3]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename bill id from dataframe so that it matches dictionary of id and status\n",
    "\n",
    "text_df['index']=text_df['index'].apply(lambda x: rename_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77560</th>\n",
       "      <td>s995-114</td>\n",
       "      <td>to establish congressional trade negotiating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77561</th>\n",
       "      <td>s996-114</td>\n",
       "      <td>to facilitate nationwide availability of volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77562</th>\n",
       "      <td>s997-114</td>\n",
       "      <td>to extend the authorization for the major med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77563</th>\n",
       "      <td>s998-114</td>\n",
       "      <td>to establish a process for the consideration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77564</th>\n",
       "      <td>s999-114</td>\n",
       "      <td>to amend the small business act to provide fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index                                               Text\n",
       "77560  s995-114   to establish congressional trade negotiating ...\n",
       "77561  s996-114   to facilitate nationwide availability of volu...\n",
       "77562  s997-114   to extend the authorization for the major med...\n",
       "77563  s998-114   to establish a process for the consideration ...\n",
       "77564  s999-114   to amend the small business act to provide fo..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "\n",
    "merged_text = pd.merge(text_df,status_df, on='index',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77564, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77565, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bill Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr1-107</td>\n",
       "      <td>a bill to close the achievement gap with acco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr10-107</td>\n",
       "      <td>to provide for pension reform, and for other ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hr100-107</td>\n",
       "      <td>to establish and expand programs relating to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hr1000-107</td>\n",
       "      <td>to adjust the boundary of the william howard ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hr1001-107</td>\n",
       "      <td>to amend title xix of the social security act...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                               Text  Bill Status\n",
       "0     hr1-107   a bill to close the achievement gap with acco...            1\n",
       "1    hr10-107   to provide for pension reform, and for other ...            1\n",
       "2   hr100-107   to establish and expand programs relating to ...            1\n",
       "3  hr1000-107   to adjust the boundary of the william howard ...            1\n",
       "4  hr1001-107   to amend title xix of the social security act...            0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_text['Text']\n",
    "y = merged_text['Bill Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text column \n",
    "documents=merged_text['Text'].tolist()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to remove stop words and tokenize\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens =[word_tokenize(doc) for doc in documents]\n",
    "\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "          for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words='english',token_pattern=r'\\b[^\\d\\W]+\\b',analyzer='word')\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaas',\n",
       " 'aace',\n",
       " 'aaces',\n",
       " 'aacn',\n",
       " 'aadt',\n",
       " 'aafp',\n",
       " 'aageson',\n",
       " 'aahca',\n",
       " 'aai',\n",
       " 'aaj',\n",
       " 'aak',\n",
       " 'aamodt',\n",
       " 'aan',\n",
       " 'aand',\n",
       " 'aaniih',\n",
       " 'aaniiih',\n",
       " 'aap',\n",
       " 'aapcc',\n",
       " 'aapi',\n",
       " 'aapis',\n",
       " 'aar',\n",
       " 'aarhus',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'aas',\n",
       " 'aasf',\n",
       " 'aasia',\n",
       " 'aatcc',\n",
       " 'aau',\n",
       " 'aawv',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'ababa',\n",
       " 'abacha',\n",
       " 'abad',\n",
       " 'abalone',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandonments',\n",
       " 'abandons',\n",
       " 'abate',\n",
       " 'abated',\n",
       " 'abatement',\n",
       " 'abatements',\n",
       " 'abates',\n",
       " 'abating',\n",
       " 'abbas',\n",
       " 'abbasid',\n",
       " 'abbeville',\n",
       " 'abbey',\n",
       " 'abbiatico',\n",
       " 'abbot',\n",
       " 'abbots',\n",
       " 'abbott',\n",
       " 'abbottabad',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abc',\n",
       " 'abcorps',\n",
       " 'abcs',\n",
       " 'abd',\n",
       " 'abdel',\n",
       " 'abdicate',\n",
       " 'abdicated',\n",
       " 'abdicating',\n",
       " 'abdication',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominalis',\n",
       " 'abdoul',\n",
       " 'abduct',\n",
       " 'abducted',\n",
       " 'abductees',\n",
       " 'abducting',\n",
       " 'abduction',\n",
       " 'abductions',\n",
       " 'abductor',\n",
       " 'abductors',\n",
       " 'abducts',\n",
       " 'abdul',\n",
       " 'abdulaziz',\n",
       " 'abdulla',\n",
       " 'abdullah',\n",
       " 'abdullahi',\n",
       " 'abdullatif',\n",
       " 'abdulmutallab',\n",
       " 'abdulqassim',\n",
       " 'abdulwahid',\n",
       " 'abdurakhmanov',\n",
       " 'abe',\n",
       " 'abecnego',\n",
       " 'abedini',\n",
       " 'abel',\n",
       " 'abend',\n",
       " 'abercorn',\n",
       " 'abercrombie',\n",
       " 'aberdeen',\n",
       " 'aberfoyle',\n",
       " 'abernathy',\n",
       " 'aberrant',\n",
       " 'aberrational',\n",
       " 'aberrations',\n",
       " 'abet',\n",
       " 'abets',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abettor',\n",
       " 'abettors',\n",
       " 'abeyance',\n",
       " 'abeyta',\n",
       " 'abhorrent',\n",
       " 'abide',\n",
       " 'abided',\n",
       " 'abides',\n",
       " 'abidine',\n",
       " 'abiding',\n",
       " 'abie',\n",
       " 'abies',\n",
       " 'abigail',\n",
       " 'abilene',\n",
       " 'abilio',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abilityone',\n",
       " 'abington',\n",
       " 'abiodun',\n",
       " 'abiotic',\n",
       " 'abiquiu',\n",
       " 'abizaid',\n",
       " 'abject',\n",
       " 'abjuration',\n",
       " 'abjuring',\n",
       " 'abkhazia',\n",
       " 'ablated',\n",
       " 'ablation',\n",
       " 'able',\n",
       " 'ableism',\n",
       " 'abler',\n",
       " 'ablerstadt',\n",
       " 'abligations',\n",
       " 'ably',\n",
       " 'abm',\n",
       " 'abmc',\n",
       " 'abms',\n",
       " 'abner',\n",
       " 'abney',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'abnormality',\n",
       " 'abnormally',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolishe',\n",
       " 'abolished',\n",
       " 'abolishes',\n",
       " 'abolishing',\n",
       " 'abolishment',\n",
       " 'abolishments',\n",
       " 'abolition',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionists',\n",
       " 'abolitions',\n",
       " 'aborad',\n",
       " 'aboriginal',\n",
       " 'aboriginally',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'abortifacient',\n",
       " 'aborting',\n",
       " 'abortion',\n",
       " 'abortionists',\n",
       " 'abortions',\n",
       " 'abortus',\n",
       " 'abottabad',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'aboveground',\n",
       " 'abovementioned',\n",
       " 'abraham',\n",
       " 'abrams',\n",
       " 'abrasion',\n",
       " 'abrasions',\n",
       " 'abreast',\n",
       " 'abridge',\n",
       " 'abridged',\n",
       " 'abridgement',\n",
       " 'abridgements',\n",
       " 'abridges',\n",
       " 'abridging',\n",
       " 'abridgment',\n",
       " 'abridgments',\n",
       " 'abrigo',\n",
       " 'abroa',\n",
       " 'abroad',\n",
       " 'abrogate',\n",
       " 'abrogated',\n",
       " 'abrogates',\n",
       " 'abrogating',\n",
       " 'abrogation',\n",
       " 'abrupt',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absaroka',\n",
       " 'abscess',\n",
       " 'abscisic',\n",
       " 'abscission',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'absconder',\n",
       " 'absconders',\n",
       " 'absconding',\n",
       " 'absconds',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absentia',\n",
       " 'absenting',\n",
       " 'absents',\n",
       " 'abshir',\n",
       " 'abshire',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolution',\n",
       " 'absolvable',\n",
       " 'absolve',\n",
       " 'absolved',\n",
       " 'absolves',\n",
       " 'absolving',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbency',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorptiometry',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptriometry',\n",
       " 'abstain',\n",
       " 'abstaining',\n",
       " 'abstention',\n",
       " 'abstentions',\n",
       " 'abstinence',\n",
       " 'abstinent',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstracting',\n",
       " 'abstraction',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abtc',\n",
       " 'abu',\n",
       " 'abubakar',\n",
       " 'abuja',\n",
       " 'abundance',\n",
       " 'abundances',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abut',\n",
       " 'abutment',\n",
       " 'abutments',\n",
       " 'abuts',\n",
       " 'abutted',\n",
       " 'abutter',\n",
       " 'abutting',\n",
       " 'abuzayd',\n",
       " 'abyei',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'abyssinian',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'acaa',\n",
       " 'acabq',\n",
       " 'acacia',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academically',\n",
       " 'academician',\n",
       " 'academicians',\n",
       " 'academics',\n",
       " 'academies',\n",
       " 'academinc',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acadiana',\n",
       " 'acai',\n",
       " 'acanthias',\n",
       " 'acarb',\n",
       " 'acas',\n",
       " 'acc',\n",
       " 'acca',\n",
       " 'accceptable',\n",
       " 'accede',\n",
       " 'acceded',\n",
       " 'accedes',\n",
       " 'acceding',\n",
       " 'accelerants',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerations',\n",
       " 'accelerator',\n",
       " 'accelerators',\n",
       " 'accent',\n",
       " 'accentuate',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptances',\n",
       " 'accepte',\n",
       " 'accepted',\n",
       " 'acceptible',\n",
       " 'accepting',\n",
       " 'acceptor',\n",
       " 'acceptors',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessability',\n",
       " 'accessed',\n",
       " 'accesser',\n",
       " 'accesses',\n",
       " 'accessibility',\n",
       " 'accessibilty',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessioning',\n",
       " 'accessions',\n",
       " 'accessorial',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidenta',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accion',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclimate',\n",
       " 'acclimatization',\n",
       " 'acclivity',\n",
       " 'accn',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accomack',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accommodative',\n",
       " 'accomodate',\n",
       " 'accomodating',\n",
       " 'accomodation',\n",
       " 'accomodations',\n",
       " 'accompained',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompaniments',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplishe',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accopatough',\n",
       " 'accops',\n",
       " 'accor',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accorded',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordion',\n",
       " 'accords',\n",
       " 'accosted',\n",
       " 'account',\n",
       " 'accountabilities',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountablity',\n",
       " 'accountably',\n",
       " 'accountancy',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accountasubpart',\n",
       " 'accounted',\n",
       " 'accountholder',\n",
       " 'accountholders',\n",
       " 'accounting',\n",
       " 'accountings',\n",
       " 'accounts',\n",
       " 'accouterments',\n",
       " 'accoutrement',\n",
       " 'accra',\n",
       " 'accredit',\n",
       " 'accreditation',\n",
       " 'accreditations',\n",
       " 'accredite',\n",
       " 'accredited',\n",
       " 'accrediting',\n",
       " 'accreditor',\n",
       " 'accreditors',\n",
       " 'accredits',\n",
       " 'accreted',\n",
       " 'accretion',\n",
       " 'accretions',\n",
       " 'accruable',\n",
       " 'accrual',\n",
       " 'accruals',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accrues',\n",
       " 'accruing',\n",
       " 'acction',\n",
       " 'acctual',\n",
       " 'acculturated',\n",
       " 'acculturation',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulates',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accumulations',\n",
       " 'accumulative',\n",
       " 'accumulator',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accurateness',\n",
       " 'accuring',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accusatory',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'accutal',\n",
       " 'accutane',\n",
       " 'acdi',\n",
       " 'ace',\n",
       " 'acec',\n",
       " 'aceh',\n",
       " 'acehnese',\n",
       " 'acela',\n",
       " 'acene',\n",
       " 'aceneth',\n",
       " 'acephate',\n",
       " 'acepromazine',\n",
       " 'acequia',\n",
       " 'acequias',\n",
       " 'acer',\n",
       " 'aces',\n",
       " 'acet',\n",
       " 'aceta',\n",
       " 'acetal',\n",
       " 'acetaldehyde',\n",
       " 'acetamide',\n",
       " 'acetamidine',\n",
       " 'acetamipr',\n",
       " 'acetamiprid',\n",
       " 'acetanisole',\n",
       " 'acetate',\n",
       " 'acetates',\n",
       " 'acetato',\n",
       " 'acetic',\n",
       " 'aceto',\n",
       " 'acetoacet',\n",
       " 'acetoacetamide',\n",
       " 'acetoacetanisidid',\n",
       " 'acetoacetanisidide',\n",
       " 'acetoacetyl',\n",
       " 'acetone',\n",
       " 'acetonitrile',\n",
       " 'acetophenone',\n",
       " 'acetoxy',\n",
       " 'acetoxyimino',\n",
       " 'acetoxyiminothioa',\n",
       " 'acetoxyiminothioace',\n",
       " 'acety',\n",
       " 'acetyl',\n",
       " 'acetylamino',\n",
       " 'acetylaminofluorene',\n",
       " 'acetylbutyrolacto',\n",
       " 'acetylbutyrolactone',\n",
       " 'acetylcholine',\n",
       " 'acetylene',\n",
       " 'acetylnicotinic',\n",
       " 'acetyloxime',\n",
       " 'acetyloxy',\n",
       " 'acetylphosphorami',\n",
       " 'acetylphosphoramidoth',\n",
       " 'acetylsalicylic',\n",
       " 'acetyltoluene',\n",
       " 'acevedo',\n",
       " 'acf',\n",
       " 'acgme',\n",
       " 'ach',\n",
       " 'acha',\n",
       " 'achaemenid',\n",
       " 'ache',\n",
       " 'acheh',\n",
       " 'aches',\n",
       " 'achievability',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achiever',\n",
       " 'achievers',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'achmad',\n",
       " 'acholi',\n",
       " 'achondroplasia',\n",
       " 'acicular',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acidification',\n",
       " 'acidified',\n",
       " 'acidifying',\n",
       " 'acidity',\n",
       " 'acids',\n",
       " 'acifluorfen',\n",
       " 'acinetobacter',\n",
       " 'acinonyx',\n",
       " 'acip',\n",
       " 'ack',\n",
       " 'ackerman',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledgements',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acknowledgments',\n",
       " 'aclu',\n",
       " 'acm',\n",
       " 'acmg',\n",
       " 'acn',\n",
       " 'acne',\n",
       " 'aco',\n",
       " 'acog',\n",
       " 'acoma',\n",
       " 'acord',\n",
       " 'acorn',\n",
       " 'acorns',\n",
       " 'acos',\n",
       " 'acosta',\n",
       " 'acota',\n",
       " 'acountability',\n",
       " 'acoustic',\n",
       " 'acoustical',\n",
       " 'acoustically',\n",
       " 'acoustics',\n",
       " 'acove',\n",
       " 'acp',\n",
       " 'acpera',\n",
       " 'acquaculture',\n",
       " 'acquaint',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquainted',\n",
       " 'acquainting',\n",
       " 'acquiesce',\n",
       " 'acquiesced',\n",
       " 'acquiescence',\n",
       " 'acquiesces',\n",
       " 'acquiescing',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquirer',\n",
       " 'acquirers',\n",
       " 'acquires',\n",
       " 'acquiring',\n",
       " 'acquis',\n",
       " 'acquisitioin',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acquistion',\n",
       " 'acquit',\n",
       " 'acquitta',\n",
       " 'acquittal',\n",
       " 'acquittals',\n",
       " 'acquittance',\n",
       " 'acquitted',\n",
       " 'acr',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acreages',\n",
       " 'acree',\n",
       " 'acres',\n",
       " 'acridine',\n",
       " 'acrimony',\n",
       " 'acrobats',\n",
       " 'acrolein',\n",
       " 'acronal',\n",
       " 'acronym',\n",
       " 'acronyms',\n",
       " 'acrrued',\n",
       " 'acrs',\n",
       " 'acrylamide',\n",
       " 'acrylate',\n",
       " 'acrylic',\n",
       " 'acrylonitrile',\n",
       " 'acryonitrile',\n",
       " 'acrypet',\n",
       " 'acs',\n",
       " 'acscan',\n",
       " 'acsr',\n",
       " 'acsubpart',\n",
       " 'act',\n",
       " 'actand',\n",
       " 'acted',\n",
       " 'actf',\n",
       " 'acting',\n",
       " 'actiniaria',\n",
       " 'actinic',\n",
       " 'actinide',\n",
       " 'actinides',\n",
       " 'actinolite',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actionably',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'activations',\n",
       " 'activator',\n",
       " 'activators',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'actives',\n",
       " 'activiation',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activit',\n",
       " 'activited',\n",
       " 'activites',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'activityk',\n",
       " 'activiy',\n",
       " 'acton',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actt',\n",
       " 'actua',\n",
       " 'actual',\n",
       " 'actualities',\n",
       " 'actuality',\n",
       " 'actualize',\n",
       " 'actualizing',\n",
       " 'actually',\n",
       " 'actuals',\n",
       " 'actuarial',\n",
       " 'actuarially',\n",
       " 'actuaries',\n",
       " 'actuarily',\n",
       " 'actuary',\n",
       " 'actuated',\n",
       " 'actuation',\n",
       " 'actuator',\n",
       " 'actuators',\n",
       " 'actuely',\n",
       " 'actvities',\n",
       " 'acu',\n",
       " 'acuff',\n",
       " 'acuity',\n",
       " 'acumen',\n",
       " 'acupuncture',\n",
       " 'acupuncturist',\n",
       " 'acupuncturists',\n",
       " 'acus',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'acuteness',\n",
       " 'acvp',\n",
       " 'acwa',\n",
       " 'acyf',\n",
       " 'acyl',\n",
       " 'acyloxy',\n",
       " 'aczm',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adab',\n",
       " 'adachi',\n",
       " 'adaes',\n",
       " 'adair',\n",
       " 'adak',\n",
       " 'adal',\n",
       " 'adam',\n",
       " 'adamantan',\n",
       " 'adams',\n",
       " 'adamsville',\n",
       " 'adan',\n",
       " 'adap',\n",
       " 'adapt',\n",
       " 'adaptability',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'adaptions',\n",
       " 'adaptive',\n",
       " 'adaptively',\n",
       " 'adaptiveness',\n",
       " 'adaptor',\n",
       " 'adaptors',\n",
       " 'adapts',\n",
       " 'adarand',\n",
       " 'adb',\n",
       " 'adbica',\n",
       " 'adcc',\n",
       " 'adco',\n",
       " 'add',\n",
       " 'addax',\n",
       " 'addded',\n",
       " 'adde',\n",
       " 'added',\n",
       " 'addeliar',\n",
       " 'addenda',\n",
       " 'addendu',\n",
       " 'addendum',\n",
       " 'addendums',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addiction',\n",
       " 'addictions',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'addie',\n",
       " 'addinc',\n",
       " 'adding',\n",
       " 'addis',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additiona',\n",
       " 'additional',\n",
       " 'additionalamounts',\n",
       " 'additionality',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'additives',\n",
       " 'additivity',\n",
       " 'additon',\n",
       " 'addley',\n",
       " 'addministration',\n",
       " 'addng',\n",
       " 'address',\n",
       " 'addressable',\n",
       " 'addresse',\n",
       " 'addressed',\n",
       " 'addressee',\n",
       " 'addressees',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addtion',\n",
       " 'addtional',\n",
       " 'adduce',\n",
       " 'adduced',\n",
       " 'adduces',\n",
       " 'adducing',\n",
       " 'adduct',\n",
       " 'adea',\n",
       " 'adebayo',\n",
       " 'adeeb',\n",
       " 'adekoya',\n",
       " 'adel',\n",
       " 'adela',\n",
       " 'adele',\n",
       " 'adelfa',\n",
       " 'adelgid',\n",
       " 'adelgids',\n",
       " 'adelphi',\n",
       " 'aden',\n",
       " 'adenoma',\n",
       " 'adentro',\n",
       " 'adept',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adfm',\n",
       " 'adh',\n",
       " 'adhamiyah',\n",
       " 'adhd',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " 'adherence',\n",
       " 'adherent',\n",
       " 'adherents',\n",
       " 'adheres',\n",
       " 'adhering',\n",
       " 'adhesion',\n",
       " 'adhesive',\n",
       " 'adhesives',\n",
       " 'adhs',\n",
       " 'adi',\n",
       " 'adicional',\n",
       " 'adil',\n",
       " 'adilene',\n",
       " 'adios',\n",
       " 'adipamide',\n",
       " 'adipic',\n",
       " 'adirondack',\n",
       " 'adirondacks',\n",
       " 'adits',\n",
       " 'adivsal',\n",
       " 'adiz',\n",
       " 'adjacency',\n",
       " 'adjacent',\n",
       " 'adjective',\n",
       " 'adjoin',\n",
       " 'adjoining',\n",
       " 'adjoins',\n",
       " 'adjourn',\n",
       " 'adjourned',\n",
       " 'adjourning',\n",
       " 'adjournment',\n",
       " 'adjournments',\n",
       " 'adjourns',\n",
       " 'adjsutment',\n",
       " 'adjudge',\n",
       " 'adjudged',\n",
       " 'adjudging',\n",
       " 'adjudicate',\n",
       " 'adjudicated',\n",
       " 'adjudicates',\n",
       " 'adjudicating',\n",
       " 'adjudication',\n",
       " 'adjudications',\n",
       " 'adjudicative',\n",
       " 'adjudicator',\n",
       " 'adjudicators',\n",
       " 'adjudicatory',\n",
       " 'adjudicting',\n",
       " 'adjunct',\n",
       " 'adjunctive',\n",
       " 'adjunctively',\n",
       " 'adjuncts',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjuste',\n",
       " 'adjusted',\n",
       " 'adjuster',\n",
       " 'adjusters',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adjustor',\n",
       " 'adjustors',\n",
       " 'adjusts',\n",
       " 'adjutant',\n",
       " 'adjutants',\n",
       " 'adjuvant',\n",
       " 'adjuvants',\n",
       " 'adkins',\n",
       " 'adl',\n",
       " 'adler',\n",
       " 'adls',\n",
       " 'adlt',\n",
       " 'admin',\n",
       " 'admininstration',\n",
       " 'administ',\n",
       " 'administation',\n",
       " 'administative',\n",
       " 'administator',\n",
       " 'administer',\n",
       " 'administere',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administers',\n",
       " 'administrability',\n",
       " 'administrable',\n",
       " 'administracion',\n",
       " 'administraion',\n",
       " 'administrate',\n",
       " 'administrated',\n",
       " 'administratieve',\n",
       " 'administrating',\n",
       " 'administration',\n",
       " 'administrationional',\n",
       " 'administrationr',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administratively',\n",
       " 'administrativo',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'administratrix',\n",
       " 'adminsitration',\n",
       " 'adminsters',\n",
       " 'adminstration',\n",
       " 'adminstrative',\n",
       " 'adminstrator',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admiralty',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admires',\n",
       " 'admiring',\n",
       " 'admissibility',\n",
       " 'admissible',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admixing',\n",
       " 'admnistrator',\n",
       " 'admonish',\n",
       " 'admonished',\n",
       " 'admonishment',\n",
       " 'admonishments',\n",
       " 'admonition',\n",
       " 'admonitions',\n",
       " 'adnexa',\n",
       " 'adobe',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adolf',\n",
       " 'adolfo',\n",
       " 'adolph',\n",
       " 'adolphus',\n",
       " 'adom',\n",
       " 'adopt',\n",
       " 'adoptability',\n",
       " 'adoptable',\n",
       " 'adopte',\n",
       " 'adopted',\n",
       " 'adoptee',\n",
       " 'adoptees',\n",
       " 'adopter',\n",
       " ...]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\b[^\\d\\W]+\\b', stop_words='english')\n",
    "\n",
    "count_vect.fit(X)\n",
    "# examine the fitted vocabulary\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Train/Test split\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=0.3)\n",
    "for train_index, test_index in stratified_split.split(X, y):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# transform matrix of plots into lists to pass to a TfidfVectorizer\n",
    "train_x = [x[0].strip() for x in x_train.tolist()]\n",
    "test_x = [x[0].strip() for x in x_test.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(test_x)\n",
    "print(xtrain_count)\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{4,}', max_features=5000, stop_words='english')\n",
    "tfidf_vect.fit(X)\n",
    "\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(410351 unique tokens: ['$1,000,000', '$1,470,000,000', '$10,000', '$10,000,000', \"$10,000,000''.\"]...)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# dct = Dictionary(X)  # fit dictionary\n",
    "# corpus = [dct.doc2bow(line) for line in X]  # convert corpus to BoW format\n",
    "# model = TfidfModel(corpus)  # fit model\n",
    "# vector = model[corpus[0]]  # apply model to the first corpus document\n",
    "\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in docs]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-599c470e8bc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# convert corpus to BoW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# apply model to the first corpus document\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-599c470e8bc2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# convert corpus to BoW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# apply model to the first corpus document\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(line) for line in X]  # convert corpus to BoW format\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "vector = model[corpus[0]]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "documents = [tokenize(reuters.raw(file_id)) for file_id in reuters.fileids()]\n",
    "dictionary = Dictionary(documents)\n",
    " \n",
    "\n",
    "tfidf_model = TfidfModel([dictionary.doc2bow(d) for d in documents], id2word=dictionary)\n",
    "tfidf_values = dict(tfidf_model[dictionary.doc2bow(tokenize(reuters.raw('test/14829')))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, Word TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t='the amendments made_by this section_ shall apply.to dispositions..after the date .... of the enactment of this act. \", \"107hr1236ih\": \" to amend the tariff suspension and trade act of 2000 to provide for the permanent designation of the san antonio international airport as an airport at which certain private aircraft arriving in the united states may land for processing. be it enacted by the senate and house of representatives of the united states of america in congress assembled, section 1. designation of san antonio international airport for customs processing of certain private aircraft arriving in the united states. section 1453 of the tariff suspension and trade act of 2000 is amended by striking for the 2-year period beginning on the date of the enactment of this act, the'' and inserting the''. \", \"107hr1237ih\": \" to designate certain lands in the valley forge national historical park as the valley forge national cemetery. be it enacted by the senate and house of representatives of the united states of america in congress assembled, section 1. designation of lands as valley forge national cemetery.  in general. not more than 200 acres of land located within the valley forge national historical park on the day before the date of the enactment of this act are hereby designated as the valley forge national cemetery. administrative jurisdiction over such lands is hereby transferred to the secretary of veterans affairs and such lands shall be administered as a national cemetery in accordance with chapter 24 of title 38, united states code (relating to national cemeteries and memorials).  adjustment of park boundaries. subsection  of section 2 of the act entitled an act to authorize the secretary of the interior to establish the valley forge national historical park in the commonwealth of pennsylvania, and for other purposes'' (16 u.s.c. 410aa-1) is amended by striking map entitled valley forge national historical park, dated june 1979, and numbered vf-91,001 and inserting map entitled valley forge national historical park, dated ____, and numbered ____'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the amendments made_by this section_ shall apply.to dispositionsafter the date  of the enactment of this act. \", \"107hr1236ih\": \" to amend the tariff suspension and trade act of 2000 to provide for the permanent designation of the san antonio international airport as an airport at which certain private aircraft arriving in the united states may land for processing. be it enacted by the senate and house of representatives of the united states of america in congress assembled, section 1. designation of san antonio international airport for customs processing of certain private aircraft arriving in the united states. section 1453 of the tariff suspension and trade act of 2000 is amended by striking for the 2-year period beginning on the date of the enactment of this act, the and inserting the. \", \"107hr1237ih\": \" to designate certain lands in the valley forge national historical park as the valley forge national cemetery. be it enacted by the senate and house of representatives of the united states of america in congress assembled, section 1. designation of lands as valley forge national cemetery.  in general. not more than 200 acres of land located within the valley forge national historical park on the day before the date of the enactment of this act are hereby designated as the valley forge national cemetery. administrative jurisdiction over such lands is hereby transferred to the secretary of veterans affairs and such lands shall be administered as a national cemetery in accordance with chapter 24 of title 38, united states code (relating to national cemeteries and memorials).  adjustment of park boundaries. subsection  of section 2 of the act entitled an act to authorize the secretary of the interior to establish the valley forge national historical park in the commonwealth of pennsylvania, and for other purposes (16 u.s.c. 410aa-1) is amended by striking map entitled valley forge national historical park, dated june 1979, and numbered vf-91,001 and inserting map entitled valley forge national historical park, dated ____, and numbered ____'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\.{2,}','',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read metadata to obtain the class labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Train/Test split\n",
    "\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=0.33)\n",
    "for train_index, test_index in stratified_split.split(data_x, data_y):\n",
    "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "\n",
    "# transform matrix of plots into lists to pass to a TfidfVectorizer\n",
    "train_x = [x[0].strip() for x in x_train.tolist()]\n",
    "test_x = [x[0].strip() for x in x_test.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf', MultinomialNB()),\n",
    "... ])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance on test set\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM\n",
    "\n",
    "\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-3, n_iter=5, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    ">>> predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    ">>> np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "\n",
    ">>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...               'tfidf__use_idf': (True, False),\n",
    "...               'clf__alpha': (1e-2, 1e-3),\n",
    "... }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_tune = GridSearchCV(\n",
    "    pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "print\n",
    "\n",
    "# measuring performance on test set\n",
    "print \"Applying best classifier on test data:\"\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "predictions = best_clf.predict(test_x)\n",
    "\n",
    "print classification_report(test_y, predictions, target_names=genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...               'tfidf__use_idf': (True, False),\n",
    "...               'clf-svm__alpha': (1e-2, 1e-3),\n",
    "... }\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a word embedding network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a vocabulary\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train doc2vec\n",
    "\n",
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build final feature vector for classifier\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressorsdef vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Logistic Regression Classifier\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding + Convolutional Neural Network\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length, weights):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tArguments\n",
    "\n",
    "\t\t---------\n",
    "\n",
    "\t\tbatch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "\n",
    "\t\toutput_size : 2 = (pos, neg)\n",
    "\n",
    "\t\tin_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n",
    "\n",
    "\t\tout_channels : Number of output channels after convolution operation performed on the input matrix\n",
    "\n",
    "\t\tkernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n",
    "\n",
    "\t\tkeep_probab : Probability of retaining an activation node during dropout operation\n",
    "\n",
    "\t\tvocab_size : Size of the vocabulary containing unique words\n",
    "\n",
    "\t\tembedding_length : Embedding dimension of GloVe word embeddings\n",
    "\n",
    "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
    "\n",
    "\t\t--------\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_heights = kernel_heights\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], embedding_length), stride, padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], embedding_length), stride, padding)\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], embedding_length), stride, padding)\n",
    "        self.dropout = nn.Dropout(keep_probab)\n",
    "        self.label = nn.Linear(len(kernel_heights)*out_channels, output_size)\n",
    "\n",
    "    def conv_block(self, input, conv_layer):\n",
    "\n",
    "        conv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "        activation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
    "\n",
    "        return max_out\n",
    "\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tThe idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix \n",
    "\n",
    "\t\twhose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n",
    "\n",
    "\t\tWe will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor \n",
    "\n",
    "\t\tand will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n",
    "\n",
    "\t\tto the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tParameters\n",
    "\n",
    "\t\t----------\n",
    "\n",
    "\t\tinput_sentences: input_sentences of shape = (batch_size, num_sequences)\n",
    "\n",
    "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tReturns\n",
    "\n",
    "\t\t-------\n",
    "\n",
    "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
    "\n",
    "\t\tlogits.size() = (batch_size, output_size)\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        # input.size() = (batch_size, num_seq, embedding_length)\n",
    "        input = input.unsqueeze(1)\n",
    "        # input.size() = (batch_size, 1, num_seq, embedding_length)\n",
    "        max_out1 = self.conv_block(input, self.conv1)\n",
    "        max_out2 = self.conv_block(input, self.conv2)\n",
    "        max_out3 = self.conv_block(input, self.conv3)\n",
    "\n",
    "\n",
    "        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "        # all_out.size() = (batch_size, num_kernels*out_channels)\n",
    "\n",
    "        fc_in = self.dropout(all_out)\n",
    "\n",
    "        # fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
    "\n",
    "        logits = self.label(fc_in)\n",
    "\n",
    "\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on new data (115th Congress)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
